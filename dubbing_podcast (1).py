# -*- coding: utf-8 -*-
"""Dubbing podcast.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_0yiweNoX7nsQ8zTpUa8BnRc1LB-PclX

**Install Packages:**
"""

!pip install simple_diarizer openai-whisper elevenlabs python-dotenv

!apt-get update && apt-get install-y ffmpeg

"""**Audio Extraction (Mono 16 kHz):**"""

!ffmpeg -y -i "/content/drive/MyDrive/video for dubbing podcast/WhatsApp Video 2025-10-15 at 10.33.42 AM.mp4" -acodec pcm_s16le -ac 1 -ar 16000 audio.wav

"""**simple diarizer:**"""

from simple_diarizer.diarizer import Diarizer

diarizer = Diarizer(embed_model='xvec', cluster_method='sc')
segments = diarizer.diarize("audio.wav", num_speakers=2)

for seg in segments:
 print(f"Speaker {seg['label']}: {seg['start']:.2f}s to {seg['end']:.2f}s")

"""**Transcribe with Whisper:**"""

import whisper

model = whisper.load_model("small.en")
result = model.transcribe("audio.wav")
trans_segments = result["segments"]

trans_segments[0:3]

"""**Map Whisper text to speakers:**"""

print(segments[0:3])

pip install --upgrade pydub

"""**transcribe with whisper:**"""

import whisper
from pydub import AudioSegment

model = whisper.load_model("small")
result = model.transcribe("audio.wav")
segments = result["segments"]

for i, seg in enumerate(segments):
    print(f"[{i}] ({seg['start']}s â†’ {seg['end']}s): {seg['text']}")

print("\n Enter speaker labels for each segment as 0 or 1, in order.")
print(f" Example for {len(segments)} segments: {'0'*len(segments)}")
label_input = input(f"Enter {len(segments)} speaker labels (0 or 1): ").strip()

if len(label_input) != len(segments) or any(c not in "01" for c in label_input):
    raise ValueError("Invalid input. Make sure you enter only 0 or 1 for each segment.")

real_speaker_ids = [int(c) for c in label_input]

"""**load full audio and dfine track:**"""

full_audio = AudioSegment.from_file("audio.wav")
audio_length_ms = len(full_audio)

speaker_0_audio = AudioSegment.silent(duration=0)
speaker_1_audio = AudioSegment.silent(duration=0)
last_end = {0: 0, 1: 0}

"""**logic for building tracks:**"""

for idx, seg in enumerate(segments):
    speaker_id = real_speaker_ids[idx]
    start_ms = int(seg["start"] * 1000)
    end_ms = int(seg["end"] * 1000)

    if end_ms > audio_length_ms:
        end_ms = audio_length_ms

    chunk = full_audio[start_ms:end_ms]
    gap = max(0, start_ms - last_end[speaker_id])
    silence = AudioSegment.silent(duration=gap)

    if speaker_id == 0:
        speaker_0_audio += silence + chunk
    else:
        speaker_1_audio += silence + chunk

    last_end[speaker_id] = start_ms + len(chunk)

for speaker_id in [0, 1]:
    track = speaker_0_audio if speaker_id == 0 else speaker_1_audio
    if len(track) < audio_length_ms:
        padding = AudioSegment.silent(duration=(audio_length_ms - len(track)))
        if speaker_id == 0:
            speaker_0_audio += padding
        else:
            speaker_1_audio += padding

"""**exposed audio**"""

speaker_0_audio.export("speaker_0.mp3", format="mp3")
speaker_1_audio.export("speaker_1.mp3", format="mp3")

print("Exported speaker_0.mp3 and speaker_1.mp3")

"""**ElevenLab for each speaker:**"""

!pip install elevenlabs --upgrade

from pydub import AudioSegment
from elevenlabs import save, VoiceSettings
from elevenlabs.client import ElevenLabs
import os

"""**add api key:**"""

API_KEY = "use user_api_key"
client = ElevenLabs(api_key=API_KEY)

"""**merge both voices:**

**voice ID mapping**
"""

speaker_voice_ids = {
    0: "JBFqnCBsd6RMkjVDRZzb",  # Speaker 0
    1: "SOYHLrjzK2X1ezoPC6cr",  # Speaker 1
}

"""**total timeline for final audio**"""

final_length_ms = int((segments[-1]["end"] + 1) * 1000)
final_audio = AudioSegment.silent(duration=final_length_ms)

print("ğŸ¤ Generating and aligning segments...")

"""**each segment and generate speaker audio**"""

for idx, (seg, speaker_id) in enumerate(zip(segments, real_speaker_ids)):
    text = seg["text"]
    start_ms = int(seg["start"] * 1000)
    end_ms = int(seg["end"] * 1000)
    duration_ms = end_ms - start_ms

    voice_id = speaker_voice_ids.get(speaker_id)

    try:
        audio_stream = client.text_to_speech.convert(
            text=text,
            voice_id=voice_id,
            voice_settings=VoiceSettings(stability=0.3, similarity_boost=0.8)
        )

        temp_path = f"temp_{idx}.mp3"
        save(audio_stream, temp_path)
        chunk = AudioSegment.from_file(temp_path)
        os.remove(temp_path)

        if len(chunk) < duration_ms:
            chunk += AudioSegment.silent(duration=(duration_ms - len(chunk)))
        else:
            chunk = chunk[:duration_ms]

        final_audio = final_audio.overlay(chunk, position=start_ms)

    except Exception as e:
        print(f" Error processing segment {idx}: {e}")

"""**Export the final result**"""

final_audio.export("dubb_audio.mp3", format="mp3")
print(" 'dubb_audio.mp3' generated with aligned speaker voices.")